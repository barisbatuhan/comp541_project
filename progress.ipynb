{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"./BBTNet/datasets/WIDERFACE.jl\")\n",
    "include(\"configs.jl\")\n",
    "\n",
    "train_data = WIDER_Data(wf_path * \"train/\")\n",
    "print(\"WIDER Data is read. Total images: \", train_data.num_files, \" & Total Faces: \", train_data.num_faces, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"./BBTNet/datasets/WIDERFACE.jl\")\n",
    "\n",
    "(imgs, boxes), state = iterate(train_data)\n",
    "print(\"Got the first batch: \", size(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"./BBTNet/utils/ImageReader.jl\")\n",
    "\n",
    "train_reader = Image_Reader(true)\n",
    "test_reader = Image_Reader(false)\n",
    "\n",
    "filename = \"0--Parade/0_Parade_marchingband_1_799.jpg\"\n",
    "img, bboxes = read_img(train_reader, root_dir * \"train/images/\" * filename, train_data.bboxes[filename], 640)\n",
    "print(\"Image file is read: \", filename, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"./BBTNet/utils/draw.jl\")\n",
    "\n",
    "draw_boxes_and_landmarks(img, bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"./BBTNet/models/retinaface.jl\")\n",
    "\n",
    "# atype = KnetArray{Float32}\n",
    "atype = Array{Float32}\n",
    "\n",
    "x = convert(atype, rand(640, 640, 3, 2))\n",
    "model = RetinaFace(dtype=atype)\n",
    "\n",
    "c, b, l = model(x, train=false)\n",
    "print(size(c), \" & \", size(b), \" & \", size(l), \" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"BBTNet/models/retinaface.jl\")\n",
    "include(\"BBTNet/datasets/WIDERFACE.jl\")\n",
    "include(\"./BBTNet/utils/draw.jl\")\n",
    "include(\"./BBTNet/utils/box_processes.jl\")\n",
    "\n",
    "atype = KnetArray{Float32}\n",
    "# atype = Array{Float32}\n",
    "images_folder_dir = \"../Datasets/WIDERFACE/WIDER_\"\n",
    "\n",
    "val_data = WIDER_Data(images_folder_dir * \"val/\", train=false, batch_size=2)\n",
    "(imgs, boxes), state = iterate(val_data)\n",
    "print(\"Got the first batch: \", size(imgs), \"\\n\")\n",
    "\n",
    "model = RetinaFace(dtype=atype)\n",
    "imgs_permuted = convert(atype, permutedims(imgs, (3,2,1,4)))\n",
    "c, b, l = model(imgs_permuted, train=false)\n",
    "\n",
    "combined = permutedims(cat(b[1], l[1], dims=2), (2, 1))\n",
    "\n",
    "draw_boxes_and_landmarks(imgs[:,:,:,1], combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2\n",
    "using FileIO\n",
    "\n",
    "@save \"retinaface_rand.jld2\" model\n",
    "model_dict = load(\"retinaface_rand.jld2\")\n",
    "model2 = model_dict[\"model\"]\n",
    "print(\"Model is loaded from a file!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Images\n",
    "\n",
    "include(\"./BBTNet/utils/draw.jl\")\n",
    "\n",
    "data = reshape([449 330 122 149 488.906 373.643 542.089 376.442 515.031 412.83 485.174 425.893 538.357 431.491], (14, 1))\n",
    "path = wf_path * \"train/images/0--Parade/0_Parade_marchingband_1_849.jpg\"\n",
    "\n",
    "img = channelview(load(path))\n",
    "draw_boxes_and_landmarks(img, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"./BBTNet/utils/ImageReader.jl\")\n",
    "\n",
    "rotated = reverse(img, dims=3)\n",
    "# colorview(RGB, float.(rotated))\n",
    "size(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random, Knet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"BBTNet/models/retinaface.jl\")\n",
    "include(\"BBTNet/datasets/WIDERFACE.jl\")\n",
    "include(\"configs.jl\")\n",
    "\n",
    "Random.seed!(42)\n",
    "\n",
    "data = WIDER_Data(wf_path * \"train/\", wf_labels_path * \"train/\", train=true, batch_size=1, dtype=atype)\n",
    "\n",
    "model = RetinaFace(dtype=atype)\n",
    "train_model(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "@primitive  fx g1 g2...\n",
       "\\end{verbatim}\n",
       "Define a new primitive operation for AutoGrad and (optionally) specify its gradients. Non-differentiable functions such as \\texttt{sign}, and non-numeric functions such as \\texttt{size} should be defined using the @zerograd macro instead.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "@primitive sin(x::Number)\n",
       "@primitive hypot(x1,x2),dy,y\n",
       "\n",
       "@primitive sin(x::Number),dy  (dy.*cos(x))\n",
       "@primitive hypot(x1,x2),dy,y  (dy.*x1./y)  (dy.*x2./y)\n",
       "\\end{verbatim}\n",
       "The first example shows that \\texttt{fx} is a typed method declaration.  Julia supports multiple dispatch, i.e. a single function can have multiple methods with different arg types. AutoGrad takes advantage of this and supports multiple dispatch for primitives and gradients.\n",
       "\n",
       "The second example specifies variable names for the output gradient \\texttt{dy} and the output \\texttt{y} after the method declaration which can be used in gradient expressions.  Untyped, ellipsis and keyword arguments are ok as in \\texttt{f(a::Int,b,c...;d=1)}.  Parametric methods such as \\texttt{f(x::T) where \\{T<:Number\\}} cannot be used.\n",
       "\n",
       "The method declaration can optionally be followed by gradient expressions.  The third and fourth examples show how gradients can be specified.  Note that the parameters, the return variable and the output gradient of the original function can be used in the gradient expressions.\n",
       "\n",
       "\\section{Under the hood}\n",
       "The @primitive macro turns the first example into:\n",
       "\n",
       "\\begin{verbatim}\n",
       "sin(x::Value{T}) where {T<:Number} = forw(sin, x)\n",
       "\\end{verbatim}\n",
       "This will cause calls to \\texttt{sin} with a boxed argument (\\texttt{Value\\{T<:Number\\}}) to be recorded. The recorded operations are used by AutoGrad to construct a dynamic computational graph. With multiple arguments things are a bit more complicated.  Here is what happens with the second example:\n",
       "\n",
       "\\begin{verbatim}\n",
       "hypot(x1::Value{S}, x2::Value{T}) where {S,T} = forw(hypot, x1, x2)\n",
       "hypot(x1::S, x2::Value{T})        where {S,T} = forw(hypot, x1, x2)\n",
       "hypot(x1::Value{S}, x2::T)        where {S,T} = forw(hypot, x1, x2)\n",
       "\\end{verbatim}\n",
       "We want the forw method to be called if any one of the arguments is a boxed \\texttt{Value}.  There is no easy way to specify this in Julia, so the macro generates all 2\\^{}N-1 boxed/unboxed argument combinations.\n",
       "\n",
       "In AutoGrad, gradients are defined using gradient methods that have the following pattern:\n",
       "\n",
       "\\begin{verbatim}\n",
       "back(f,Arg{i},dy,y,x...) => dx[i]\n",
       "\\end{verbatim}\n",
       "For the third example here is the generated gradient method:\n",
       "\n",
       "\\begin{verbatim}\n",
       "back(::typeof(sin), ::Type{Arg{1}}, dy, y, x::Value{T}) where {T<:Number} = dy .* cos(x)\n",
       "\\end{verbatim}\n",
       "For the last example a different gradient method is generated for each argument:\n",
       "\n",
       "\\begin{verbatim}\n",
       "back(::typeof(hypot), ::Type{Arg{1}}, dy, y, x1::Value{S}, x2::Value{T}) where {S,T} = (dy .* x1) ./ y\n",
       "back(::typeof(hypot), ::Type{Arg{2}}, dy, y, x1::Value{S}, x2::Value{T}) where {S,T} = (dy .* x2) ./ y\n",
       "\\end{verbatim}\n",
       "In fact @primitive generates four more definitions for the other boxed/unboxed argument combinations.\n",
       "\n",
       "\\section{Broadcasting}\n",
       "Broadcasting is handled by extra \\texttt{forw} and \\texttt{back} methods. \\texttt{@primitive} defines the following  so that broadcasting of a primitive function with a boxed value triggers \\texttt{forw} and \\texttt{back}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "broadcasted(::typeof(sin), x::Value{T}) where {T<:Number} = forw(broadcasted,sin,x)\n",
       "back(::typeof(broadcasted), ::Type{Arg{2}}, dy, y, ::typeof(sin), x::Value{T}) where {T<:Number} = dy .* cos(x)\n",
       "\\end{verbatim}\n",
       "If you do not want the broadcasting methods, you can use the \\texttt{@primitive1} macro. If you only want the broadcasting methods use \\texttt{@primitive2}. As a motivating example, here is how \\texttt{*} is defined for non-scalars:\n",
       "\n",
       "\\begin{verbatim}\n",
       "@primitive1 *(x1,x2),dy  (dy*x2')  (x1'*dy)\n",
       "@primitive2 *(x1,x2),dy  unbroadcast(x1,dy.*x2)  unbroadcast(x2,x1.*dy)\n",
       "\\end{verbatim}\n",
       "Regular \\texttt{*} is matrix multiplication, broadcasted \\texttt{*} is elementwise multiplication and the two have different gradients as defined above. \\texttt{unbroadcast(a,b)} reduces \\texttt{b} to the same shape as \\texttt{a} by performing the necessary summations.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "@primitive  fx g1 g2...\n",
       "```\n",
       "\n",
       "Define a new primitive operation for AutoGrad and (optionally) specify its gradients. Non-differentiable functions such as `sign`, and non-numeric functions such as `size` should be defined using the @zerograd macro instead.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```\n",
       "@primitive sin(x::Number)\n",
       "@primitive hypot(x1,x2),dy,y\n",
       "\n",
       "@primitive sin(x::Number),dy  (dy.*cos(x))\n",
       "@primitive hypot(x1,x2),dy,y  (dy.*x1./y)  (dy.*x2./y)\n",
       "```\n",
       "\n",
       "The first example shows that `fx` is a typed method declaration.  Julia supports multiple dispatch, i.e. a single function can have multiple methods with different arg types. AutoGrad takes advantage of this and supports multiple dispatch for primitives and gradients.\n",
       "\n",
       "The second example specifies variable names for the output gradient `dy` and the output `y` after the method declaration which can be used in gradient expressions.  Untyped, ellipsis and keyword arguments are ok as in `f(a::Int,b,c...;d=1)`.  Parametric methods such as `f(x::T) where {T<:Number}` cannot be used.\n",
       "\n",
       "The method declaration can optionally be followed by gradient expressions.  The third and fourth examples show how gradients can be specified.  Note that the parameters, the return variable and the output gradient of the original function can be used in the gradient expressions.\n",
       "\n",
       "# Under the hood\n",
       "\n",
       "The @primitive macro turns the first example into:\n",
       "\n",
       "```\n",
       "sin(x::Value{T}) where {T<:Number} = forw(sin, x)\n",
       "```\n",
       "\n",
       "This will cause calls to `sin` with a boxed argument (`Value{T<:Number}`) to be recorded. The recorded operations are used by AutoGrad to construct a dynamic computational graph. With multiple arguments things are a bit more complicated.  Here is what happens with the second example:\n",
       "\n",
       "```\n",
       "hypot(x1::Value{S}, x2::Value{T}) where {S,T} = forw(hypot, x1, x2)\n",
       "hypot(x1::S, x2::Value{T})        where {S,T} = forw(hypot, x1, x2)\n",
       "hypot(x1::Value{S}, x2::T)        where {S,T} = forw(hypot, x1, x2)\n",
       "```\n",
       "\n",
       "We want the forw method to be called if any one of the arguments is a boxed `Value`.  There is no easy way to specify this in Julia, so the macro generates all 2^N-1 boxed/unboxed argument combinations.\n",
       "\n",
       "In AutoGrad, gradients are defined using gradient methods that have the following pattern:\n",
       "\n",
       "```\n",
       "back(f,Arg{i},dy,y,x...) => dx[i]\n",
       "```\n",
       "\n",
       "For the third example here is the generated gradient method:\n",
       "\n",
       "```\n",
       "back(::typeof(sin), ::Type{Arg{1}}, dy, y, x::Value{T}) where {T<:Number} = dy .* cos(x)\n",
       "```\n",
       "\n",
       "For the last example a different gradient method is generated for each argument:\n",
       "\n",
       "```\n",
       "back(::typeof(hypot), ::Type{Arg{1}}, dy, y, x1::Value{S}, x2::Value{T}) where {S,T} = (dy .* x1) ./ y\n",
       "back(::typeof(hypot), ::Type{Arg{2}}, dy, y, x1::Value{S}, x2::Value{T}) where {S,T} = (dy .* x2) ./ y\n",
       "```\n",
       "\n",
       "In fact @primitive generates four more definitions for the other boxed/unboxed argument combinations.\n",
       "\n",
       "# Broadcasting\n",
       "\n",
       "Broadcasting is handled by extra `forw` and `back` methods. `@primitive` defines the following  so that broadcasting of a primitive function with a boxed value triggers `forw` and `back`.\n",
       "\n",
       "```\n",
       "broadcasted(::typeof(sin), x::Value{T}) where {T<:Number} = forw(broadcasted,sin,x)\n",
       "back(::typeof(broadcasted), ::Type{Arg{2}}, dy, y, ::typeof(sin), x::Value{T}) where {T<:Number} = dy .* cos(x)\n",
       "```\n",
       "\n",
       "If you do not want the broadcasting methods, you can use the `@primitive1` macro. If you only want the broadcasting methods use `@primitive2`. As a motivating example, here is how `*` is defined for non-scalars:\n",
       "\n",
       "```\n",
       "@primitive1 *(x1,x2),dy  (dy*x2')  (x1'*dy)\n",
       "@primitive2 *(x1,x2),dy  unbroadcast(x1,dy.*x2)  unbroadcast(x2,x1.*dy)\n",
       "```\n",
       "\n",
       "Regular `*` is matrix multiplication, broadcasted `*` is elementwise multiplication and the two have different gradients as defined above. `unbroadcast(a,b)` reduces `b` to the same shape as `a` by performing the necessary summations.\n"
      ],
      "text/plain": [
       "\u001b[36m  @primitive  fx g1 g2...\u001b[39m\n",
       "\n",
       "  Define a new primitive operation for AutoGrad and (optionally) specify its\n",
       "  gradients. Non-differentiable functions such as \u001b[36msign\u001b[39m, and non-numeric\n",
       "  functions such as \u001b[36msize\u001b[39m should be defined using the @zerograd macro instead.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  @primitive sin(x::Number)\u001b[39m\n",
       "\u001b[36m  @primitive hypot(x1,x2),dy,y\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  @primitive sin(x::Number),dy  (dy.*cos(x))\u001b[39m\n",
       "\u001b[36m  @primitive hypot(x1,x2),dy,y  (dy.*x1./y)  (dy.*x2./y)\u001b[39m\n",
       "\n",
       "  The first example shows that \u001b[36mfx\u001b[39m is a typed method declaration. Julia\n",
       "  supports multiple dispatch, i.e. a single function can have multiple methods\n",
       "  with different arg types. AutoGrad takes advantage of this and supports\n",
       "  multiple dispatch for primitives and gradients.\n",
       "\n",
       "  The second example specifies variable names for the output gradient \u001b[36mdy\u001b[39m and\n",
       "  the output \u001b[36my\u001b[39m after the method declaration which can be used in gradient\n",
       "  expressions. Untyped, ellipsis and keyword arguments are ok as in\n",
       "  \u001b[36mf(a::Int,b,c...;d=1)\u001b[39m. Parametric methods such as \u001b[36mf(x::T) where {T<:Number}\u001b[39m\n",
       "  cannot be used.\n",
       "\n",
       "  The method declaration can optionally be followed by gradient expressions.\n",
       "  The third and fourth examples show how gradients can be specified. Note that\n",
       "  the parameters, the return variable and the output gradient of the original\n",
       "  function can be used in the gradient expressions.\n",
       "\n",
       "\u001b[1m  Under the hood\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  The @primitive macro turns the first example into:\n",
       "\n",
       "\u001b[36m  sin(x::Value{T}) where {T<:Number} = forw(sin, x)\u001b[39m\n",
       "\n",
       "  This will cause calls to \u001b[36msin\u001b[39m with a boxed argument (\u001b[36mValue{T<:Number}\u001b[39m) to be\n",
       "  recorded. The recorded operations are used by AutoGrad to construct a\n",
       "  dynamic computational graph. With multiple arguments things are a bit more\n",
       "  complicated. Here is what happens with the second example:\n",
       "\n",
       "\u001b[36m  hypot(x1::Value{S}, x2::Value{T}) where {S,T} = forw(hypot, x1, x2)\u001b[39m\n",
       "\u001b[36m  hypot(x1::S, x2::Value{T})        where {S,T} = forw(hypot, x1, x2)\u001b[39m\n",
       "\u001b[36m  hypot(x1::Value{S}, x2::T)        where {S,T} = forw(hypot, x1, x2)\u001b[39m\n",
       "\n",
       "  We want the forw method to be called if any one of the arguments is a boxed\n",
       "  \u001b[36mValue\u001b[39m. There is no easy way to specify this in Julia, so the macro generates\n",
       "  all 2^N-1 boxed/unboxed argument combinations.\n",
       "\n",
       "  In AutoGrad, gradients are defined using gradient methods that have the\n",
       "  following pattern:\n",
       "\n",
       "\u001b[36m  back(f,Arg{i},dy,y,x...) => dx[i]\u001b[39m\n",
       "\n",
       "  For the third example here is the generated gradient method:\n",
       "\n",
       "\u001b[36m  back(::typeof(sin), ::Type{Arg{1}}, dy, y, x::Value{T}) where {T<:Number} = dy .* cos(x)\u001b[39m\n",
       "\n",
       "  For the last example a different gradient method is generated for each\n",
       "  argument:\n",
       "\n",
       "\u001b[36m  back(::typeof(hypot), ::Type{Arg{1}}, dy, y, x1::Value{S}, x2::Value{T}) where {S,T} = (dy .* x1) ./ y\u001b[39m\n",
       "\u001b[36m  back(::typeof(hypot), ::Type{Arg{2}}, dy, y, x1::Value{S}, x2::Value{T}) where {S,T} = (dy .* x2) ./ y\u001b[39m\n",
       "\n",
       "  In fact @primitive generates four more definitions for the other\n",
       "  boxed/unboxed argument combinations.\n",
       "\n",
       "\u001b[1m  Broadcasting\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  Broadcasting is handled by extra \u001b[36mforw\u001b[39m and \u001b[36mback\u001b[39m methods. \u001b[36m@primitive\u001b[39m defines\n",
       "  the following so that broadcasting of a primitive function with a boxed\n",
       "  value triggers \u001b[36mforw\u001b[39m and \u001b[36mback\u001b[39m.\n",
       "\n",
       "\u001b[36m  broadcasted(::typeof(sin), x::Value{T}) where {T<:Number} = forw(broadcasted,sin,x)\u001b[39m\n",
       "\u001b[36m  back(::typeof(broadcasted), ::Type{Arg{2}}, dy, y, ::typeof(sin), x::Value{T}) where {T<:Number} = dy .* cos(x)\u001b[39m\n",
       "\n",
       "  If you do not want the broadcasting methods, you can use the \u001b[36m@primitive1\u001b[39m\n",
       "  macro. If you only want the broadcasting methods use \u001b[36m@primitive2\u001b[39m. As a\n",
       "  motivating example, here is how \u001b[36m*\u001b[39m is defined for non-scalars:\n",
       "\n",
       "\u001b[36m  @primitive1 *(x1,x2),dy  (dy*x2')  (x1'*dy)\u001b[39m\n",
       "\u001b[36m  @primitive2 *(x1,x2),dy  unbroadcast(x1,dy.*x2)  unbroadcast(x2,x1.*dy)\u001b[39m\n",
       "\n",
       "  Regular \u001b[36m*\u001b[39m is matrix multiplication, broadcasted \u001b[36m*\u001b[39m is elementwise\n",
       "  multiplication and the two have different gradients as defined above.\n",
       "  \u001b[36munbroadcast(a,b)\u001b[39m reduces \u001b[36mb\u001b[39m to the same shape as \u001b[36ma\u001b[39m by performing the\n",
       "  necessary summations."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using AutoGrad\n",
    "\n",
    "@doc @primitive1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: Package Pycall not found in current path:\n- Run `import Pkg; Pkg.add(\"Pycall\")` to install the Pycall package.\n",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package Pycall not found in current path:\n- Run `import Pkg; Pkg.add(\"Pycall\")` to install the Pycall package.\n",
      "",
      "Stacktrace:",
      " [1] require(::Module, ::Symbol) at ./loading.jl:893",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "using Pycall\n",
    "\n",
    "@pyimport numpy \n",
    "@pyimport torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.add(\"Pycall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×5 Array{Float64,2}:\n",
       " 0.463706  -1.29645  -1.42806    -0.183817  0.320075\n",
       " 0.310392   0.61776   0.212497    0.661844  0.632299\n",
       " 0.102909   1.31899  -0.0961019  -0.413573  0.0163056"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = randn(3, 5)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{CartesianIndex{2},1}:\n",
       " CartesianIndex(2, 2)\n",
       " CartesianIndex(3, 2)\n",
       " CartesianIndex(2, 4)\n",
       " CartesianIndex(2, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = findall(temp .> 0.5)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching !(::Array{CartesianIndex{2},1})\nClosest candidates are:\n  !(!Matched::Missing) at missing.jl:100\n  !(!Matched::Bool) at bool.jl:33\n  !(!Matched::Function) at operators.jl:896",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching !(::Array{CartesianIndex{2},1})\nClosest candidates are:\n  !(!Matched::Missing) at missing.jl:100\n  !(!Matched::Bool) at bool.jl:33\n  !(!Matched::Function) at operators.jl:896",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[14]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "temp[!idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WIDER_Data(\"/datasets/widerface/WIDER_train/\", Dict{Any,Any}(\"25--Soldier_Patrol/25_Soldier_Patrol_Soldier_Patrol_25_331.jpg\" => [380.0; 96.0; … ; 191.0; 1.0],\"14--Traffic/14_Traffic_Traffic_14_811.jpg\" => [558.0; 108.0; … ; -1.0; -1.0],\"3--Riot/3_Riot_Riot_3_177.jpg\" => [277.0 224.0 … 582.0 187.0; 122.0 33.0 … 44.0 46.0; … ; 173.938 -1.0 … -1.0 -1.0; 1.0 -1.0 … -1.0 -1.0],\"35--Basketball/35_Basketball_playingbasketball_35_69.jpg\" => [620.0; 52.0; … ; 161.076; 1.0],\"0--Parade/0_Parade_Parade_0_142.jpg\" => [808.0 928.0 … 392.0 237.0; 236.0 219.0 … 51.0 52.0; … ; 277.625 258.34 … -1.0 -1.0; 1.0 1.0 … -1.0 -1.0],\"24--Soldier_Firing/24_Soldier_Firing_Soldier_Firing_24_972.jpg\" => [378.0; 137.0; … ; 264.067; 1.0],\"41--Swimming/41_Swimming_Swimmer_41_150.jpg\" => [670.0; 208.0; … ; 286.589; 1.0],\"12--Group/12_Group_Large_Group_12_Group_Large_Group_12_516.jpg\" => [417.0 498.0; 429.0 366.0; … ; 514.754 468.46; 1.0 1.0],\"12--Group/12_Group_Team_Organized_Group_12_Group_Team_Organized_Group_12_433.jpg\" => [480.0 56.0; 339.0 976.0; … ; 465.304 1046.411; 1.0 1.0],\"51--Dresses/51_Dresses_wearingdress_51_634.jpg\" => [881.0 581.0 353.0 72.0; 22.0 66.0 69.0 10.0; … ; 68.955 113.589 108.214 59.02; 1.0 1.0 1.0 1.0]…), [\"0--Parade/0_Parade_marchingband_1_849.jpg\", \"0--Parade/0_Parade_Parade_0_904.jpg\", \"0--Parade/0_Parade_marchingband_1_799.jpg\", \"0--Parade/0_Parade_marchingband_1_117.jpg\", \"0--Parade/0_Parade_marchingband_1_778.jpg\", \"0--Parade/0_Parade_Parade_0_343.jpg\", \"0--Parade/0_Parade_marchingband_1_205.jpg\", \"0--Parade/0_Parade_Parade_0_106.jpg\", \"0--Parade/0_Parade_Parade_0_476.jpg\", \"0--Parade/0_Parade_marchingband_1_12.jpg\"  …  \"9--Press_Conference/9_Press_Conference_Press_Conference_9_358.jpg\", \"9--Press_Conference/9_Press_Conference_Press_Conference_9_940.jpg\", \"9--Press_Conference/9_Press_Conference_Press_Conference_9_893.jpg\", \"9--Press_Conference/9_Press_Conference_Press_Conference_9_925.jpg\", \"9--Press_Conference/9_Press_Conference_Press_Conference_9_472.jpg\", \"9--Press_Conference/9_Press_Conference_Press_Conference_9_614.jpg\", \"9--Press_Conference/9_Press_Conference_Press_Conference_9_791.jpg\", \"9--Press_Conference/9_Press_Conference_Press_Conference_9_280.jpg\", \"9--Press_Conference/9_Press_Conference_Press_Conference_9_131.jpg\", \"9--Press_Conference/9_Press_Conference_Press_Conference_9_266.jpg\"], 16, 12880, 159424, true, true, 1, KnetArray{Float32,N} where N)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"BBTNet/datasets/WIDERFACE.jl\")\n",
    "include(\"configs.jl\")\n",
    "\n",
    "import Random\n",
    "Random.seed!(42)\n",
    "data = WIDER_Data(wf_path * \"train/\", wf_labels_path * \"train/\", train=true, batch_size=batch_size, dtype=atype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== TRAINING PROCESS ==============================\n",
      "\n",
      "(640, 640, 3, 16)(16,)\n",
      "\n",
      "Stacktrace:\n",
      " [1] \u001b[1malloc\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/CUDA/YeS8q/src/pool.jl:298\u001b[22m [inlined]\n",
      " [2] \u001b[1mCUDA.CuArray{UInt8,1}\u001b[22m\u001b[1m(\u001b[22m::UndefInitializer, ::Tuple{Int64}\u001b[1m)\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/CUDA/YeS8q/src/array.jl:20\u001b[22m\n",
      " [3] \u001b[1mCuArray\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/CUDA/YeS8q/src/array.jl:76\u001b[22m [inlined]\n",
      " [4] \u001b[1mCuArray\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/CUDA/YeS8q/src/array.jl:77\u001b[22m [inlined]\n",
      " [5] \u001b[1mKnetPtrCu\u001b[22m\u001b[1m(\u001b[22m::Int64\u001b[1m)\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/Knet/C0PoK/src/knetarrays/kptr.jl:229\u001b[22m\n",
      " [6] \u001b[1mKnet.KnetArrays.KnetPtr\u001b[22m\u001b[1m(\u001b[22m::Int64\u001b[1m)\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/Knet/C0PoK/src/knetarrays/kptr.jl:107\u001b[22m\n",
      " [7] \u001b[1mKnetArray\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/Knet/C0PoK/src/knetarrays/karray.jl:75\u001b[22m [inlined]\n",
      " [8] \u001b[1msimilar\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/Knet/C0PoK/src/knetarrays/abstractarray.jl:28\u001b[22m [inlined]\n",
      " [9] \u001b[1msimilar\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/Knet/C0PoK/src/knetarrays/abstractarray.jl:25\u001b[22m [inlined]\n",
      " [10] \u001b[1mconv4\u001b[22m\u001b[1m(\u001b[22m::KnetArray{Float32,4}, ::KnetArray{Float32,4}; handle::Ptr{Nothing}, alpha::Int64, o::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:padding, :stride, :dilation),Tuple{Int64,Int64,Int64}}}\u001b[1m)\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/Knet/C0PoK/src/ops20_gpu/conv.jl:8\u001b[22m\n",
      " [11] \u001b[1mforw\u001b[22m\u001b[1m(\u001b[22m::Function, ::Param{KnetArray{Float32,4}}, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:padding, :stride, :dilation),Tuple{Int64,Int64,Int64}}}\u001b[1m)\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/AutoGrad/VFrAv/src/core.jl:66\u001b[22m\n",
      " [12] \u001b[1m#conv4#22\u001b[22m at \u001b[1m./none:0\u001b[22m [inlined]\n",
      " [13] \u001b[1m(::Conv2D)\u001b[22m\u001b[1m(\u001b[22m::AutoGrad.Result{KnetArray{Float32,4}}; train::Bool\u001b[1m)\u001b[22m at \u001b[1m/scratch/users/baristopal20/retinaface/BBTNet/core/layers.jl:79\u001b[22m\n",
      " [14] \u001b[1m(::Residual_1x3x1)\u001b[22m\u001b[1m(\u001b[22m::AutoGrad.Result{KnetArray{Float32,4}}; train::Bool\u001b[1m)\u001b[22m at \u001b[1m/scratch/users/baristopal20/retinaface/BBTNet/core/blocks.jl:66\u001b[22m\n",
      " [15] \u001b[1m(::Chain)\u001b[22m\u001b[1m(\u001b[22m::AutoGrad.Result{KnetArray{Float32,4}}; train::Bool\u001b[1m)\u001b[22m at \u001b[1m/scratch/users/baristopal20/retinaface/BBTNet/core/blocks.jl:78\u001b[22m\n",
      " [16] \u001b[1m(::ResNet50)\u001b[22m\u001b[1m(\u001b[22m::KnetArray{Float32,4}; train::Bool, return_intermediate::Bool\u001b[1m)\u001b[22m at \u001b[1m/scratch/users/baristopal20/retinaface/BBTNet/backbones/resnet.jl:48\u001b[22m\n",
      " [17] \u001b[1m(::RetinaFace)\u001b[22m\u001b[1m(\u001b[22m::KnetArray{Float32,4}, ::Array{Any,1}, ::Int64, ::Bool, ::Float64\u001b[1m)\u001b[22m at \u001b[1m/scratch/users/baristopal20/retinaface/BBTNet/models/retinaface.jl:75\u001b[22m\n",
      " [18] \u001b[1m(::Knet.Train20.var\"#27#28\"{Knet.Train20.Minimize{Array{Tuple{KnetArray{Float32,4},Array{Any,1},Int64,Bool,Float64},1}},Tuple{KnetArray{Float32,4},Array{Any,1},Int64,Bool,Float64}})\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/AutoGrad/VFrAv/src/core.jl:205\u001b[22m\n",
      " [19] \u001b[1mdifferentiate\u001b[22m\u001b[1m(\u001b[22m::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}\u001b[1m)\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/AutoGrad/VFrAv/src/core.jl:144\u001b[22m\n",
      " [20] \u001b[1mdifferentiate\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/AutoGrad/VFrAv/src/core.jl:135\u001b[22m [inlined]\n",
      " [21] \u001b[1miterate\u001b[22m\u001b[1m(\u001b[22m::Knet.Train20.Minimize{Array{Tuple{KnetArray{Float32,4},Array{Any,1},Int64,Bool,Float64},1}}\u001b[1m)\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/Knet/C0PoK/src/train20/train.jl:26\u001b[22m\n",
      " [22] \u001b[1mmomentum!\u001b[22m\u001b[1m(\u001b[22m::RetinaFace, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Symbol,Float64,Tuple{Symbol,Symbol},NamedTuple{(:lr, :gamma),Tuple{Float64,Float64}}}\u001b[1m)\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/Knet/C0PoK/src/train20/update.jl:181\u001b[22m\n",
      " [23] \u001b[1mtrain_model\u001b[22m\u001b[1m(\u001b[22m::RetinaFace, ::WIDER_Data; val_data::Nothing\u001b[1m)\u001b[22m at \u001b[1m/scratch/users/baristopal20/retinaface/BBTNet/models/retinaface.jl:195\u001b[22m\n",
      " [24] \u001b[1mtrain_model\u001b[22m\u001b[1m(\u001b[22m::RetinaFace, ::WIDER_Data\u001b[1m)\u001b[22m at \u001b[1m/scratch/users/baristopal20/retinaface/BBTNet/models/retinaface.jl:181\u001b[22m\n",
      " [25] top-level scope at \u001b[1mIn[11]:4\u001b[22m\n",
      " [26] \u001b[1minclude_string\u001b[22m\u001b[1m(\u001b[22m::Function, ::Module, ::String, ::String\u001b[1m)\u001b[22m at \u001b[1m./loading.jl:1091\u001b[22m\n",
      " [27] \u001b[1msoftscope_include_string\u001b[22m\u001b[1m(\u001b[22m::Module, ::String, ::String\u001b[1m)\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/SoftGlobalScope/u4UzH/src/SoftGlobalScope.jl:65\u001b[22m\n",
      " [28] \u001b[1mexecute_request\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/IJulia/IDNmS/src/execute_request.jl:67\u001b[22m\n",
      " [29] \u001b[1m#invokelatest#1\u001b[22m at \u001b[1m./essentials.jl:710\u001b[22m [inlined]\n",
      " [30] \u001b[1minvokelatest\u001b[22m at \u001b[1m./essentials.jl:709\u001b[22m [inlined]\n",
      " [31] \u001b[1meventloop\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m at \u001b[1m/kuacc/users/baristopal20/.julia/packages/IJulia/IDNmS/src/eventloop.jl:8\u001b[22m\n",
      " [32] \u001b[1m(::IJulia.var\"#15#18\")\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1m./task.jl:356\u001b[22m\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "Out of GPU memory trying to allocate 200.000 MiB\nEffective GPU memory usage: 99.67% (14.707 GiB/14.756 GiB)\nCUDA allocator usage: 14.054 GiB\nbinned usage: 4.047 KiB (4.047 KiB allocated, 0 bytes cached)\nDiscrepancy of 14.054 GiB between memory pool and allocator!\n",
     "output_type": "error",
     "traceback": [
      "Out of GPU memory trying to allocate 200.000 MiB\nEffective GPU memory usage: 99.67% (14.707 GiB/14.756 GiB)\nCUDA allocator usage: 14.054 GiB\nbinned usage: 4.047 KiB (4.047 KiB allocated, 0 bytes cached)\nDiscrepancy of 14.054 GiB between memory pool and allocator!\n",
      "",
      "Stacktrace:",
      " [1] differentiate(::Function; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /kuacc/users/baristopal20/.julia/packages/AutoGrad/VFrAv/src/core.jl:148",
      " [2] differentiate at /kuacc/users/baristopal20/.julia/packages/AutoGrad/VFrAv/src/core.jl:135 [inlined]",
      " [3] iterate(::Knet.Train20.Minimize{Array{Tuple{KnetArray{Float32,4},Array{Any,1},Int64,Bool,Float64},1}}) at /kuacc/users/baristopal20/.julia/packages/Knet/C0PoK/src/train20/train.jl:26",
      " [4] momentum!(::RetinaFace, ::Vararg{Any,N} where N; o::Base.Iterators.Pairs{Symbol,Float64,Tuple{Symbol,Symbol},NamedTuple{(:lr, :gamma),Tuple{Float64,Float64}}}) at /kuacc/users/baristopal20/.julia/packages/Knet/C0PoK/src/train20/update.jl:181",
      " [5] train_model(::RetinaFace, ::WIDER_Data; val_data::Nothing) at /scratch/users/baristopal20/retinaface/BBTNet/models/retinaface.jl:195",
      " [6] train_model(::RetinaFace, ::WIDER_Data) at /scratch/users/baristopal20/retinaface/BBTNet/models/retinaface.jl:181",
      " [7] top-level scope at In[11]:4",
      " [8] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "include(\"BBTNet/models/retinaface.jl\")\n",
    "\n",
    "model = RetinaFace(dtype=atype)\n",
    "train_model(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mNo Matches\u001b[22m\u001b[39m in `/scratch/users/baristopal20/.julia/environments/v1.5/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "\n",
    "Pkg.status(\"PyCall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
